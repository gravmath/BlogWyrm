{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d27aedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib        as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53beab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_graph(child,new_parent,graph):\n",
    "    #get the child's old parent\n",
    "    old_parent = graph[child]\n",
    "    \n",
    "    #safety valve to limit the while loop\n",
    "    graph_size = len(graph)\n",
    "    counter    = 0\n",
    "    \n",
    "    #assuming that the new_parent is further up in the tree then\n",
    "    #we simply loop until the child equals the new_parent\n",
    "    #which is the state when a node self-refers, which is a property of it\n",
    "    #being a top node in the tree\n",
    "    while child != new_parent and counter < graph_size:\n",
    "        graph[child] = new_parent\n",
    "        child        = old_parent\n",
    "        old_parent   = graph[child]\n",
    "        counter += 1\n",
    "\n",
    "def find_root(k,proper_labels):\n",
    "    #final clean up for those proper labels that weren't \n",
    "    #collapsed when there is a linker\n",
    "    counter = 0\n",
    "    while(proper_labels[k] != k and counter < len(proper_labels)):\n",
    "        k = proper_labels[k]\n",
    "        counter = counter + 1\n",
    "    return k\n",
    "\n",
    "def init_site_labels(rows,cols):\n",
    "    proper_labels  = {-1:0,0:0}\n",
    "    cluster_labels = {}\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            cluster_labels[i*cols + j] = -1\n",
    "    return cluster_labels, proper_labels  \n",
    "\n",
    "\n",
    "def hoshen_kopleman(lattice):\n",
    "    largest_label = 1\n",
    "    rows, cols    = lattice.shape\n",
    "\n",
    "    cluster_labels, proper_labels = init_site_labels(rows,cols)\n",
    "\n",
    "    #forward sweep\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            curr              = lattice[i,j]\n",
    "            curr_linear_index = i*cols + j\n",
    "            if curr != 0:  #current cell occupied - only do the work if so\n",
    "                up_index          = max(0,i-1)\n",
    "                left_index        = max(0,j-1)\n",
    "                up_linear_index   = up_index*cols + j     if i != 0 else 0\n",
    "                left_linear_index = i*cols + left_index   if j != 0 else 0\n",
    "                up                = lattice[up_index,j]   if i != 0 else 0\n",
    "                left              = lattice[i,left_index] if j != 0 else 0\n",
    "                if up == 0 and left == 0: #site is an island (so far)\n",
    "                    proper_labels[largest_label]      = largest_label\n",
    "                    cluster_labels[curr_linear_index] = largest_label\n",
    "                    largest_label       += 1\n",
    "                elif up != 0 and left == 0: #site linked to the one above\n",
    "                    cluster_labels[curr_linear_index] = cluster_labels[up_linear_index]\n",
    "                elif up == 0 and left != 0: #site linked to the one to the left\n",
    "                    cluster_labels[curr_linear_index] = cluster_labels[left_linear_index]\n",
    "                elif up != 0 and left != 0: #site is a linker\n",
    "                    up_proper_label         = proper_labels[cluster_labels[up_linear_index]]\n",
    "                    left_proper_label       = proper_labels[cluster_labels[left_linear_index]]\n",
    "                    root_cluster_label      = min(up_proper_label,left_proper_label)\n",
    "                    proximate_cluster_label = max(up_proper_label,left_proper_label)\n",
    "                    cluster_labels[curr_linear_index] = proximate_cluster_label\n",
    "                    update_graph(proximate_cluster_label,root_cluster_label,proper_labels)\n",
    "\n",
    "\n",
    "    #collapse the proper labels so that there is only one hop to the top of the tree\n",
    "    collapsed_proper_labels = {}\n",
    "    for k in proper_labels:\n",
    "        collapsed_proper_labels[k] = find_root(k,proper_labels)\n",
    "\n",
    "    return cluster_labels, collapsed_proper_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b547d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_lattice(mat):\n",
    "    #determine the size of the lattice\n",
    "    L, M = mat.shape\n",
    "    \n",
    "    #run HK on the lattice\n",
    "    cluster_labels, proper_labels = hoshen_kopleman(mat)\n",
    "    \n",
    "    #find the unique cluster labels and the number of unique clusters\n",
    "    set_unique_cluster_labels = set(proper_labels.values())\n",
    "    num_unique_clusters       = len(set_unique_cluster_labels)\n",
    "    \n",
    "    #make a hash/association called 'translate' that simply renumbers\n",
    "    #the unique cluster labels in sequential order.  This is needed\n",
    "    #when a union forces a later cluster label to be skipped when it is \n",
    "    #discovered that that cluster is actually part of a previous one\n",
    "    translate = {}\n",
    "    for i,k in enumerate(set_unique_cluster_labels):\n",
    "        translate[k] = i\n",
    "    \n",
    "    #allocate an array for cluster labels - same size of the lattice but is\n",
    "    #flattened to be a 1-D array\n",
    "    Cs = np.zeros(L*M,dtype=int)\n",
    "\n",
    "    #load the various Cs components with the proper cluster label\n",
    "    #and then reform as a lattice\n",
    "    for i,k in enumerate(cluster_labels): \n",
    "        Cs[i] = translate[proper_labels[cluster_labels[k]]]\n",
    "    Cs = Cs.reshape(L,M)    \n",
    "    \n",
    "    #check each cluster (by scanning of the 'c' clusters found) for a \n",
    "    #spanner defined as a cluster that either goes in a path between \n",
    "    #left and right edges or top and bottom ones\n",
    "    n_s         = {}\n",
    "    s_s         = {}\n",
    "    P_inf       = 0 \n",
    "    num_spanner = 0\n",
    "    num_occ     = 0    \n",
    "    for c in range(1,np.max(Cs)+1):\n",
    "        #initially assume no spanning cluster\n",
    "        span_flag        = 0\n",
    "        #find the location of the current cluster with label 'c'\n",
    "        cluster_spots    = np.where(Cs==c)\n",
    "        #find the size of the current cluster with label 'c'\n",
    "        c_size           = len(cluster_spots[0])\n",
    "        #add to the total number of occupied sites the number of sites in\n",
    "        #the current cluster with label 'c'\n",
    "        num_occ          += num_occ + c_size\n",
    "        #find the rows and columns covered by the current cluster with label 'c'\n",
    "        rows             = cluster_spots[0]\n",
    "        cols             = cluster_spots[1]\n",
    "        #find the length and height of the current cluster with label 'c'\n",
    "        row_span         = max(rows) - min(rows)\n",
    "        col_span         = max(cols) - min(cols)\n",
    "        #calculate the center of mass (rowsb,colsb) and the radius of gyration (s)\n",
    "        rowsb            = rows - np.mean(rows)\n",
    "        colsb            = cols - np.mean(cols)\n",
    "        s                = np.sqrt(np.sum(rowsb*rowsb+colsb*colsb)/len(rows))        \n",
    "        #check to see if it is a spanning cluster and set 'P_inf' appropriately\n",
    "        if row_span == L - 1 and col_span == M - 1:\n",
    "            #row & col  spanner\n",
    "            num_spanner += 1\n",
    "            P_inf       += c_size\n",
    "            span_flag    = 1\n",
    "        elif row_span == L - 1:\n",
    "            #row spanner\n",
    "            num_spanner += 1\n",
    "            P_inf       += c_size\n",
    "            span_flag    = 1\n",
    "        elif col_span == M - 1:\n",
    "            #col spanner\n",
    "            num_spanner += 1\n",
    "            P_inf       += c_size\n",
    "            span_flag    = 1\n",
    "        #create a label to distinguish spanning clusters from ordinary ones\n",
    "        decorated_c_size = str(c_size)+'s'+str(span_flag)\n",
    "        #accumulate 'n_s' which is not really the theoretical 'n(s)'\n",
    "        if decorated_c_size in n_s.keys():\n",
    "            n_s[decorated_c_size] += 1\n",
    "        else:\n",
    "            n_s[decorated_c_size] = 1\n",
    "        #accumulate 's(s)'\n",
    "        if decorated_c_size in s_s.keys():\n",
    "            s_s[decorated_c_size] += s\n",
    "        else:\n",
    "            s_s[decorated_c_size] = s\n",
    "   \n",
    "    return num_spanner, P_inf/num_occ, n_s, Cs, s_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e30001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_n_s(trials):\n",
    "    #find out the size of the lattice\n",
    "    L, M = trials[0][3].shape\n",
    "    \n",
    "    n_s = {}\n",
    "    num_trials = len(trials)\n",
    "    for n in range(num_trials):\n",
    "        curr_n_s = trials[n][2]  #third return val of analyze matrix is the n_s\n",
    "        for k in curr_n_s.keys():\n",
    "            if k in n_s.keys():\n",
    "                n_s[k] += curr_n_s[k]/L/M/num_trials\n",
    "            else:\n",
    "                n_s[k] = curr_n_s[k]/L/M/num_trials\n",
    "                \n",
    "    return n_s #average numbers of clusters of size s\n",
    "\n",
    "def unpack_n_s_data_to_arrays(n_s):\n",
    "    sizes = []\n",
    "    nums  = []\n",
    "    for k in n_s.keys():\n",
    "        temp = k.split('s')\n",
    "        if int(temp[1]) == 0:\n",
    "            sizes.append(int(temp[0]))\n",
    "            nums.append(n_s[k])\n",
    "        \n",
    "    return np.array(sizes), np.array(nums)\n",
    "\n",
    "def collect_data_over_N_trials(p_init,num_trials,L):\n",
    "    Qs    = {}\n",
    "    Pinfs = []\n",
    "    for i in range(num_trials):\n",
    "        mat        = np.zeros((L,L))\n",
    "        probs      = np.random.rand(L,L)\n",
    "        trues      = np.where(probs <= p_init)\n",
    "        mat[trues] = 1\n",
    "        temp       = analyze_lattice(mat)\n",
    "        Qs[i] = (temp[0],temp[1],temp[2],temp[3],temp[4])\n",
    "        Pinfs.append(temp[1])\n",
    "        \n",
    "    n_s = combine_n_s(Qs)\n",
    "    \n",
    "    return unpack_n_s_data_to_arrays(n_s), np.array(Pinfs)/num_trials, Qs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d09d4b",
   "metadata": {},
   "source": [
    "1. $ P_{\\infty} \\sim (p-p_c)^\\beta $\n",
    "1. $ S(p) \\sim |p-p_c|^{-\\gamma} $\n",
    "1. $ \\xi(p) \\sim |p-p_c|^{-\\nu} $\n",
    "1. $ n_s \\sim s^{-\\tau} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9525981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lattice_size = 8\n",
    "num_samples  = 50\n",
    "A_vlo,B_vlo,C_vlo = collect_data_over_N_trials(0.4   ,num_samples,lattice_size)\n",
    "A_lo, B_lo, C_lo  = collect_data_over_N_trials(0.5   ,num_samples,lattice_size)\n",
    "A_c,  B_c,  C_c   = collect_data_over_N_trials(0.5927,num_samples,lattice_size)\n",
    "A_hi, B_hi, C_hi  = collect_data_over_N_trials(0.7   ,num_samples,lattice_size)\n",
    "A_vhi,B_vhi,C_vhi = collect_data_over_N_trials(0.8   ,num_samples,lattice_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2758db95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\byecs\\\\OneDrive\\\\Documents\\\\BlogWyrm'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bbc9130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data     = {'vlo':{'A':A_vlo,'B':B_vlo,'C':C_vlo},\n",
    "            'lo' :{'A':A_lo, 'B':B_lo, 'C':C_lo},\n",
    "            'c'  :{'A':A_c,  'B':B_c,  'C':C_c},\n",
    "            'hi' :{'A':A_hi, 'B':B_hi, 'C':C_hi},\n",
    "            'vhi':{'A':A_vhi,'B':B_vhi,'C':C_vhi}}\n",
    "with open('data_8.pkl', 'wb') as file: \n",
    "    pickle.dump(data, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d8c032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
